---
title: Sairam Tabibu's work experience
layout: default
excerpt: All the companies and places Sairam Tabibu has worked at, and the jobs
permalink: /experience
---

[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/iiith.png">](http://cvit.iiit.ac.in/){:target="_blank"}

_November 2017 --- present_

### [CVIT, IIIT Hyderabad](http://cvit.iiit.ac.in/){:target="_blank"}

**Research Fellow**, supervisor: [Prof. C.V. Jawahar](https://faculty.iiit.ac.in/~jawahar/){:target="_blank"}

<br />


# EDUCATION

[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/iitbhu.png">](http://www.iitbhu.ac.in/){:target="_blank"}

_2013 --- 2017_

### [Indian Institute of Technology, (BHU), Varanasi](http://www.iitbhu.ac.in/){:target="_blank"}, India

Bachelors Degree in Electronics Engineering

<br />


# RESEARCH PROJECTS

[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/ntu.png">](http://www.ntu.edu.sg/Pages/home.aspx){:target="_blank"}

_January-2017 --- April-2017_

### Lexical and Visual Analysis to Identify Posts Warranting Empathetic Responses

[**NTU, Singapore**](http://www.ntu.edu.sg/Pages/home.aspx) --- under [Prof. Erik Cambria](http://sentic.net/erikcambria/){:target="_blank"}, School of Computer Science and Engineering

We worked on understanding the sentiment that a person elicits on different posts present on different social media sites, on the topics of abuse or mental health. We propose a method supported by hand-crafted features to judge if the post requires an empathetic response. The model is trained upon posts from various web-pages
and corresponding comments, on both the captions and the Images. We were able to obtain 80% accuracy in tagging posts requiring empathetic responses.

A [research paper]({{site.url}}{{site.baseurl}}/docs/publications/FLAIRS.pdf){:target="_blank"} based on this work has been published in [AAAI Publications](https://aaai.org/ocs/index.php/FLAIRS/FLAIRS17/paper/view/15505){:target="_blank"} in proceedings of the Proceedings of the Thirtieth International Florida Artificial Intelligence Research Society Conference, [FLAIRS 2017](https://aaai.org/ocs/index.php/FLAIRS/FLAIRS17/paper/view/15505){:target="_blank"}.

| [**Research paper**]({{site.url}}{{site.baseurl}}/docs/publications/FLAIRS.pdf){:target="_blank"}: Jaiswal, Mimansa, Sairam Tabibu, and Erik Cambria. "“Hang in There”: Lexical and Visual Analysis to Identify Posts Warranting Empathetic Responses." (2017). |


<br />

[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/kgp.jpg">](http://www.iitkgp.ac.in/){:target="_blank"}

_2012 --- 2013_

### Identification of Bilabial Consonants in Audio and Lip Closures in Video --- B.Tech. Thesis

[**IIT Kharagpur, India**](http://www.iitkgp.ac.in/) --- under [Prof. Rajiv Ranjan Sahay](http://www1.iitkgp.ac.in/fac-profiles/showprofile.php?empcode=STmUU&depts_name=EE){:target="_blank"}, Electrical Engineering

I worked on the identification of bilabial consonants in video and audio. The goal was to measure the time offset between the two modes using corresponding time points where bilabials occur. I learnt C++ and the OpenCV library, and detected lip closures in video using the standard Viola-Jones face detector, and a novel algorithm for lip closure detection. I trained a Gaussian Mixture Model in MATLAB on the MFCC features of bilabials in the speech signals of different speakers. A correlation was drawn between the time points of bilabials in audio and video.

[THESIS](https://github.com/voletiv/BTP_GMM_lipClosure/blob/master/Bachelors_Thesis.pdf){:target="_blank"} | [Presentation](https://github.com/voletiv/BTP_GMM_lipClosure/blob/master/Vikram_Voleti_\%5B09EE3501\%5D_BTP_Presentation.pdf){:target="_blank"} | [GitHub](https://github.com/voletiv/BTP_GMM_lipClosure){:target="_blank"} repository containing thesis, presentation, code files, and results

<br />

# RESEARCH INTERNSHIPS

[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/leuven.png">](https://www.kuleuven.be/english/){:target="_blank"}

_Summer 2013_

### Implementation of Carry-Free Arithmetic Operations in FPGA

[**KU Leuven, Belgium**](https://www.kuleuven.be/english/) --- under [Prof. Ingrid Verbauwhede](https://www.kuleuven.be/wieiswie/en/person/00018159){:target="_blank"}, Computer Security & Industrial Applications, ESAT

I worked on the carry-free implementations of arithmetic operations of addition, subtraction and multiplication. Binary numbers are first converted to a recoded digit format that eliminates carry propagation. I designed the truth tables for this conversion, as well as subsequent addition, subtraction and multiplication. I then simplified the circuits into Product-of-Sums form, and coded them in Verilog. The time taken by these circuits were compared with standard implementation.

A [single-author research paper]({{site.url}}{{site.baseurl}}/docs/publications/2018_NCC.pdf){:target="_blank"} based on this work has been written.

| [**Research paper**]({{site.url}}{{site.baseurl}}/docs/publications/2018_NCC.pdf){:target="_blank"}: V. Voleti, "Carry-Free Implementations of Arithmetic Operations in FPGA" |

[Report](https://github.com/voletiv/summer_2013_KULeuven/blob/master/Leuven_Report/KULeuven_Report.pdf){:target="_blank"} | [Presentation](https://github.com/voletiv/summer_2013_KULeuven/blob/master/Leuven_Presentation/Implementation_of_Carry-Free_Arithmetic_Primitives_for_Prime_Field_Elliptic_Curve_Cryptography.pdf){:target="_blank"} | [GitHub](https://github.com/voletiv/summer_2013_KULeuven){:target="_blank"} repository containing report and presentation

<br />

[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/kgp.jpg">](http://www.iitkgp.ac.in/){:target="_blank"}

_Summer 2012_

### Fingertip Gesture Recognizer using HMMs

[**IIT Kharagpur, India**](http://www.iitkgp.ac.in/) --- under [Prof. Aurobinda Routray](http://www.aroutray.org/){:target="_blank"}, Electrical Engineering

I first implemented Hidden Markov Models (HMM) in MATLAB from scratch, and verified the implementation outputs with those of standard implementation. I then made a simple gesture recognizer in MATLAB using HMMs.

[Report](https://github.com/voletiv/summer_2012_HMM_FingerTipGestureRecognition/blob/master/Vikram\%20Voleti\%20\%5B09EE3501\%5D\%20Summer\%202012\%20Internship\%20Report.pdf){:target="_blank"} | [Presentation](https://github.com/voletiv/summer_2012_HMM_FingerTipGestureRecognition/blob/master/Ppt.pdf){:target="_blank"} | [GitHub](https://github.com/voletiv/summer_2012_HMM_FingerTipGestureRecognition){:target="_blank"} repository containing report, presentation, code files, and results

<br />

[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/imperial.jpg">](https://www.imperial.ac.uk/){:target="_blank"}

_Summer 2011_

### Measurement of Intra-die Power Variation in Sub-nm FPGA’s

[**Imperial College, London**](https://www.imperial.ac.uk/) --- under [Prof. Peter Cheung](http://www.imperial.ac.uk/people/p.cheung){:target="_blank"}, Electrical and Electronic Engineering

I experimented with an FPGA, and measured the power consumption among the LookUp Tables (LUTs) within it. An automated workflow for the measurement of power across the FPGA was made, by first implementing a circuit in each LUT, measuring the power on an oscilloscope using the JTAG terminals on the FPGA, recording the oscilloscope's readings in MATLAB, and plotting graphs from MATLAB.

[Presentation](https://github.com/voletiv/summer_2011_FPGA_Imperial_College_London/blob/master/An Automated Flow for Intra-Die Power Variation Measurement.pdf){:target="_blank"} | [GitHub](https://github.com/voletiv/summer_2011_FPGA_Imperial_College_London){:target="_blank"} repository containing presentation, certificate, and recommendation letter from Prof. Peter Cheung


